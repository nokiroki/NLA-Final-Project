{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2285fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg as splin\n",
    "import scipy.sparse as sparse\n",
    "import json\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import ndcg_score as ndcg\n",
    "from sklearn.metrics import recall_score as recall\n",
    "# from ignite.metrics.recall import Recall as t_recall\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1b04a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2975121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-3668c1e60c8a>:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_ratings = pd.read_csv('ml-1m/ratings.dat', sep=\"::\", header=None)\n"
     ]
    }
   ],
   "source": [
    "dict_df_rating = dict()\n",
    "\n",
    "#20 millions\n",
    "\n",
    "# #links = pd.read_csv('ml-20m/links.csv')\n",
    "# df_ratings = pd.read_csv('ml-20m/ratings.csv')\n",
    "# df_ratings.columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "# df_ratings.head()\n",
    "\n",
    "\n",
    "#1 million MovieLens \n",
    "\n",
    "df_ratings = pd.read_csv('ml-1m/ratings.dat', sep=\"::\", header=None)\n",
    "df_ratings.columns = ['user_id', 'object_id', 'rating', 'timestamp']\n",
    "df_ratings.drop('timestamp', axis='columns', inplace=True)\n",
    "df_ratings.dropna(inplace=True)\n",
    "df_ratings['user_id'] = df_ratings['user_id'].astype(\"category\").cat.codes\n",
    "df_ratings['object_id'] = df_ratings['object_id'].astype(\"category\").cat.codes\n",
    "\n",
    "dict_df_rating['ML1M'] = dict()\n",
    "dict_df_rating['ML1M']['original'] = df_ratings\n",
    "\n",
    "# #Books http://www2.informatik.uni-freiburg.de/~cziegler/BX/\n",
    "\n",
    "# df_ratings = pd.read_csv('Books Data/BX-Book-Ratings.csv', sep=';')\n",
    "# df_ratings.columns = ['user_id', 'object_id', 'rating']\n",
    "# df_ratings.dropna(inplace=True)\n",
    "# df_ratings['user_id'] = df_ratings['user_id'].astype(\"category\").cat.codes\n",
    "# df_ratings['object_id'] = df_ratings['object_id'].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# dict_df_rating['BX'] = dict()\n",
    "# dict_df_rating['BX']['original'] = df_ratings\n",
    "\n",
    "# #Music  http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "# df_ratings = pd.read_csv('Digital music/ratings_Digital_Music.csv', sep=',', header=None)\n",
    "# df_ratings.columns = ['user_id', 'object_id', 'rating', 'timestamp']\n",
    "# df_ratings.drop('timestamp', axis='columns', inplace=True)\n",
    "# df_ratings.dropna(inplace=True)\n",
    "# df_ratings['user_id'] = df_ratings['user_id'].astype(\"category\").cat.codes\n",
    "# df_ratings['object_id'] = df_ratings['object_id'].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# dict_df_rating['DM'] = dict()\n",
    "# dict_df_rating['DM']['original'] = df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae9ce2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparcity for ML1M dataset: 0.044683625622312845\n"
     ]
    }
   ],
   "source": [
    "for ds in dict_df_rating:\n",
    "    df_ratings = dict_df_rating[ds]['original']\n",
    "    all_cells = df_ratings.object_id.nunique() * df_ratings.user_id.nunique()\n",
    "    sparcity_rate = len(df_ratings) / all_cells\n",
    "    print(f'sparcity for {ds} dataset: {sparcity_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38c186e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ds in dict_df_rating:\n",
    "#     print(ds)\n",
    "    \n",
    "#     df_ratings = dict_df_rating[ds]['original'].copy()\n",
    "    \n",
    "#     df_ratings['user_id'] =  df_ratings['user_id'].astype('str')\n",
    "#     df_ratings['object_id'] =  df_ratings['object_id'].astype('str')\n",
    "\n",
    "#     dense_rating = df_ratings.pivot(index='user_id', columns='object_id', values='rating')\n",
    "#     dense_rating = dense_rating.fillna(0)\n",
    "#     dict_df_rating[ds]['dense'] = dense_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38257a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forming sparse matrixes\n",
    "\n",
    "for ds in dict_df_rating:\n",
    "    \n",
    "    df_ratings = dict_df_rating[ds]['original'].copy()\n",
    "\n",
    "#     dict_obj = {obj:i for i, obj in enumerate(sorted(df_ratings.object_id.unique()))}\n",
    "#     dict_user = {obj:i for i, obj in enumerate(sorted(df_ratings.user_id.unique()))}\n",
    "\n",
    "#     df_ratings['user_id'] = df_ratings['user_id'].astype(\"category\").cat.codes\n",
    "#     df_ratings['object_id'] = df_ratings['object_id'].astype(\"category\").cat.codes\n",
    "    \n",
    "#     dict_df_rating[ds]['original'] = df_ratings\n",
    "    \n",
    "    rows = df_ratings['user_id'].astype(\"int\")\n",
    "    cols = df_ratings['object_id'].astype(\"int\")\n",
    "\n",
    "    ratings = df_ratings.rating.astype(\"int\")\n",
    "    \n",
    "    data_sparse = sparse.csr_matrix((ratings, (rows, cols)), shape=(df_ratings.user_id.nunique(), \n",
    "                                                                df_ratings.object_id.nunique()), dtype='int32')\n",
    "    \n",
    "    dict_df_rating[ds]['sparse'] = data_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2054f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML1M (6040, 3706)\n"
     ]
    }
   ],
   "source": [
    "for ds in dict_df_rating:\n",
    "    print(ds, dict_df_rating[ds]['sparse'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59977498",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99c1f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, data_sparse, user_vecs, item_vecs):\n",
    "    \"\"\"Recommend items for a given user given a trained model\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): The id of the user we want to create recommendations for.\n",
    "        \n",
    "        data_sparse (csr_matrix): Our original training data.\n",
    "        \n",
    "        user_vecs (csr_matrix): The trained user x features vectors\n",
    "        \n",
    "        item_vecs (csr_matrix): The trained item x features vectors\n",
    "        \n",
    "        item_lookup (pandas.DataFrame): Used to map artist ids to artist names\n",
    "        \n",
    "        num_items (int): How many recommendations we want to return:\n",
    "        \n",
    "    Returns:\n",
    "        recommendations (pandas.DataFrame): DataFrame with num_items artist names and scores\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    # Get all interactions by the user\n",
    "    user_interactions = data_sparse[user_id,:].toarray()\n",
    "\n",
    "    # We don't want to recommend items the user has consumed. So let's\n",
    "    # set them all to 0 and the unknowns to 1.\n",
    "    user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "    #user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    # This is where we calculate the recommendation by taking the \n",
    "    # dot-product of the user vectors with the item vectors.\n",
    "    rec_vector = (user_vecs[user_id,:] @ item_vecs.T).toarray()\n",
    "\n",
    "    # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1, 1))[:,0]\n",
    "    recommend_vector = user_interactions * rec_vector_scaled\n",
    "   \n",
    "    # Get all the artist indices in order of recommendations (descending) and\n",
    "    # select only the top \"num_items\" items. \n",
    "    item_idx = np.argsort(recommend_vector)[::-1]\n",
    "\n",
    "    objects = []\n",
    "    scores = []\n",
    "\n",
    "    # Loop through our recommended artist indicies and look up the actial artist name\n",
    "    for idx in item_idx:\n",
    "        objects.append(idx)\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    # Create a new dataframe with recommended artist names and scores\n",
    "    recommendations = pd.DataFrame({'object_id': objects, 'score': scores})\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "389d4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41c5d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrix_res_dict = dict()\n",
    "\n",
    "for ds in dict_df_rating:\n",
    "    all_metrix_res = pd.DataFrame([], index = ['Recall20', 'Recall50', 'NDCG100'])\n",
    "    all_metrix_res_dict[ds] = all_metrix_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926012e",
   "metadata": {},
   "source": [
    "## ALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "65d57d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "    \"\"\" Implementation of Alternating Least Squares with implicit data. We iteratively\n",
    "    compute the user (x_u) and item (y_i)from tor vectors using the following formulas:\n",
    " \n",
    "    x_u = ((Y.T*Y + Y.T*(Cu - I) * Y) + lambda*I)^-1 * (X.T * Cu * p(u))\n",
    "    y_i = ((X.T*X + X.T*(Ci - I) * X) + lambda*I)^-1 * (Y.T * Ci * p(i))\n",
    " \n",
    "    Args:\n",
    "        sparse_data (csr_matrix): Our sparse user-by-item matrix\n",
    " \n",
    "        alpha_val (int): The rate in which we'll increase our confidence\n",
    "        in a preference with more interactions.\n",
    " \n",
    "        iterations (int): How many times we alternate between fixing and \n",
    "        updating our user and item vectors\n",
    " \n",
    "        lambda_val (float): Regularization value\n",
    " \n",
    "        features (int): How many latent features we want to compute.\n",
    "    \n",
    "    Returns:     \n",
    "        X (csr_matrix): user vectors of size users-by-features\n",
    "        \n",
    "        Y (csr_matrix): item vectors of size items-by-features\n",
    "     \"\"\"\n",
    "\n",
    "    assert type(sparse_data) == sparse.csr_matrix, \"Matrix should be sparse in format of csr\"\n",
    "    \n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "    \n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "\n",
    "    I = sparse.eye(features)\n",
    "    lI = lambda_val * I\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        yTy = Y.T @ Y\n",
    "        xTx = X.T @ X\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "            u_row = sparse_data[u,:].toarray()\n",
    "            X[u] = spsolve(yTy + lI, (u_row @ Y).T)\n",
    "\n",
    "        for i in range(item_size):\n",
    "            i_row = sparse_data[:,i].T.toarray()\n",
    "            Y[i] = spsolve(xTx + lI, (i_row @ X).T)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "258b0359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a5d55b7ed24997b5772fed6163dbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "als_res_dict = dict()\n",
    "\n",
    "for ds in dict_df_rating:\n",
    "    sparse_local = dict_df_rating[ds]['sparse']\n",
    "    user_vecs, item_vecs = implicit_als(sparse_local, iterations=5, features=200, alpha_val=40)\n",
    "    als_res_dict[ds] = (user_vecs, item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "741f27b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15abc0972c214538a90adf38abf62e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-775719cd9f59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdense_ratings_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_ratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcompilation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_ratings_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'object_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#dense_ratings_user.join(recommendations, on='object_id', how='inner')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcompilation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mcompilation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         llabels, rlabels = _items_overlap_with_suffix(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             )\n\u001b[0;32m    908\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;34m\"\"\" return the join indexers \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         return get_join_indexers(\n\u001b[0m\u001b[0;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1415\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m     )\n\u001b[1;32m-> 1417\u001b[1;33m     \u001b[0mzipped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1418\u001b[0m     \u001b[0mllab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzipped\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \u001b[1;31m# get left & right join labels and num. of levels at each location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m     mapped = (\n\u001b[1;32m-> 1414\u001b[1;33m         \u001b[0m_factorize_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_keys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_keys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_factorize_keys\u001b[1;34m(lk, rk, sort, how)\u001b[0m\n\u001b[0;32m   2056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m     \u001b[0mllab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2058\u001b[1;33m     \u001b[0mrlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ds in dict_df_rating:\n",
    "    df_ratings = dict_df_rating[ds]['original']\n",
    "    all_users = df_ratings.user_id.unique()\n",
    "    user_vecs_loc, item_vecs_loc = als_res_dict[ds]\n",
    "    \n",
    "    rec20_list = []\n",
    "    rec50_list = []\n",
    "    NDCG_list = []\n",
    "    \n",
    "    for user_id in tqdm(all_users):\n",
    "        recommendations = recommend(user_id, data_sparse, user_vecs_loc, item_vecs_loc)\n",
    "        dense_ratings_user = df_ratings[(df_ratings['user_id'] == user_id) & (df_ratings['rating'] > 0)]\n",
    "        \n",
    "        compilation = pd.merge(dense_ratings_user, recommendations, how='inner', on = 'object_id')\n",
    "        #dense_ratings_user.join(recommendations, on='object_id', how='inner')\n",
    "        compilation['score']  = compilation.score*5\n",
    "        compilation['score_round'] = round(compilation.score).astype(int)\n",
    "        \n",
    "        rec20 = recall(compilation.rating.values, compilation.score_round.values, k=20)\n",
    "        rec50 = recall(compilation.rating.values, compilation.score_round.values, k=50)\n",
    "        NDCG = ndcg(compilation.rating.values.reshape((1, -1)), compilation.score.values.reshape((1, -1)), k=100)\n",
    "        \n",
    "        rec20_list.append(rec20)\n",
    "        rec50_list.append(rec50)\n",
    "        NDCG_list.append(NDCG)\n",
    "        \n",
    "    mertix = []\n",
    "    for lst in [rec20_list, rec50_list, NDCG_list]:\n",
    "        mertix.append(np.mean(lst))\n",
    "        \n",
    "    all_metrix_res_dict[ds]['ALS'] = mertix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrix_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b82976",
   "metadata": {},
   "source": [
    "## ALS++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "19766b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alspp(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "    \"\"\" Implementation of Alternating Least Squares with implicit data. We iteratively\n",
    "    compute the user (x_u) and item (y_i)from tor vectors using the following formulas:\n",
    " \n",
    "    x_u = ((Y.T*Y + Y.T*(Cu - I) * Y) + lambda*I)^-1 * (X.T * Cu * p(u))\n",
    "    y_i = ((X.T*X + X.T*(Ci - I) * X) + lambda*I)^-1 * (Y.T * Ci * p(i))\n",
    " \n",
    "    Args:\n",
    "        sparse_data (csr_matrix): Our sparse user-by-item matrix\n",
    " \n",
    "        alpha_val (int): The rate in which we'll increase our confidence\n",
    "        in a preference with more interactions.\n",
    " \n",
    "        iterations (int): How many times we alternate between fixing and \n",
    "        updating our user and item vectors\n",
    " \n",
    "        lambda_val (float): Regularization value\n",
    " \n",
    "        features (int): How many latent features we want to compute.\n",
    "    \n",
    "    Returns:     \n",
    "        X (csr_matrix): user vectors of size users-by-features\n",
    "        \n",
    "        Y (csr_matrix): item vectors of size items-by-features\n",
    "     \"\"\"\n",
    "\n",
    "    assert type(sparse_data) == sparse.csr_matrix, \"Matrix should be sparse in format of csr\"\n",
    "    \n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "    \n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "    \n",
    "#     I = sparse.csr_matrix((np.ones(features), (np.ones(features), np.ones(features))), \n",
    "#                            shape=(features, features), dtype='int32')\n",
    "#     #I = sparse.eye(features)\n",
    "#     lI = lambda_val * I\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        print(Y.shape)\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        ind_rand = list(set(random.choices(list(range(features)), weights=None, k=features//2)))\n",
    "        Y_rand = Y[:, ind_rand]\n",
    "        X_rand = X[:, ind_rand]\n",
    "        \n",
    "        lI = lambda_val * sparse.eye(len(ind_rand))\n",
    "        \n",
    "        yTy = Y_rand.T @ Y_rand\n",
    "        xTx = X_rand.T @ X_rand\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "            u_row = sparse_data[u,:].toarray()\n",
    "            X[u, ind_rand] = spsolve(yTy + lI, (u_row @ Y_rand).T)\n",
    "\n",
    "        for i in range(item_size):\n",
    "            i_row = sparse_data[:,i].T.toarray()\n",
    "            Y[i, ind_rand] = spsolve(xTx + lI, (i_row @ X_rand).T)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3d856672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68978b76c61d4d79834c99a59a9acfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3706, 200)\n",
      "(3706, 200)\n",
      "(3706, 200)\n",
      "(3706, 200)\n",
      "(3706, 200)\n"
     ]
    }
   ],
   "source": [
    "alspp_res_dict = dict()\n",
    "\n",
    "for ds in dict_df_rating:\n",
    "    sparse_local = dict_df_rating[ds]['sparse']\n",
    "    user_vecs, item_vecs = alspp(sparse_local, iterations=5, features=200, alpha_val=40)\n",
    "    alspp_res_dict[ds] = (user_vecs, item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ca48d69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba1f1af6322410eaf2077d18bd6d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Recall20</th>\n",
       "      <td>0.193438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall50</th>\n",
       "      <td>0.308082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG100</th>\n",
       "      <td>0.972673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ALS\n",
       "Recall20  0.193438\n",
       "Recall50  0.308082\n",
       "NDCG100   0.972673"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ds in dict_df_rating:\n",
    "    df_ratings = dict_df_rating[ds]['original']\n",
    "    all_users = df_ratings.user_id.unique()\n",
    "    user_vecs_loc, item_vecs_loc = als_res_dict[ds]\n",
    "    \n",
    "    rec20_list = []\n",
    "    rec50_list = []\n",
    "    NDCG_list = []\n",
    "    \n",
    "    for user_id in tqdm(all_users):\n",
    "        recommendations = recommend(user_id, data_sparse, user_vecs_loc, item_vecs_loc)\n",
    "        dense_ratings_user = df_ratings[(df_ratings['user_id'] == user_id) & (df_ratings['rating'] > 0)]\n",
    "        \n",
    "        compilation = pd.merge(dense_ratings_user, recommendations, how='inner', on = 'object_id')\n",
    "        #dense_ratings_user.join(recommendations, on='object_id', how='inner')\n",
    "        compilation['score']  = compilation.score*5\n",
    "        compilation['score_round'] = round(compilation.score).astype(int)\n",
    "        \n",
    "        rec20 = recall(compilation.rating.values, compilation.score_round.values, k=20)\n",
    "        rec50 = recall(compilation.rating.values, compilation.score_round.values, k=50)\n",
    "        NDCG = ndcg(compilation.rating.values.reshape((1, -1)), compilation.score.values.reshape((1, -1)), k=100)\n",
    "        \n",
    "        rec20_list.append(rec20)\n",
    "        rec50_list.append(rec50)\n",
    "        NDCG_list.append(NDCG)\n",
    "        \n",
    "    mertix = []\n",
    "    for lst in [rec20_list, rec50_list, NDCG_list]:\n",
    "        mertix.append(np.mean(lst))\n",
    "        \n",
    "    all_metrix_res_dict[ds]['ALS'] = mertix\n",
    "\n",
    "all_metrix_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1121b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
